{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19e38ee4",
   "metadata": {},
   "source": [
    "### 1.  sigmoid 함수 구현 \n",
    "* 딥러닝에서는 matrix(행렬), vector가 input이기 때문에 1개의 숫자를 받는 \"math\" 라이브러리보다 \"numpy\"라이브러리를 주로 사용함\n",
    "* numpy 함수에 vector로 입력값을 넣으면, input size 그대로 output 생성 가능 \n",
    "* $sigmoid(x) = \\frac{1}{1+e^{-x}}$ \n",
    "<img src=\"images/Sigmoid.png\" style=\"width:500px;height:228px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92bd1bed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T06:49:54.898431Z",
     "start_time": "2023-05-09T06:49:54.881384Z"
    }
   },
   "outputs": [],
   "source": [
    "# math 라이브러리 이용해서 sigmoid 함수 구현 \n",
    "import math\n",
    "\n",
    "def sigmoid_using_math(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1/(1+math.exp(-x))\n",
    "    return s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9e95d8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T06:49:55.070175Z",
     "start_time": "2023-05-09T06:49:55.057164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실수 1개 test:  0.52497918747894\n"
     ]
    }
   ],
   "source": [
    "# 실수 1개 test -> 정상 동작 \n",
    "print(\"실수 1개 test: \",sigmoid_using_math(0.1))\n",
    "# vector test -> 에러 발생 \n",
    "# print(\"vector test: \",sigmoid_using_math(np.array([0.1, 0.5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71ab9862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T06:49:55.289969Z",
     "start_time": "2023-05-09T06:49:55.281275Z"
    }
   },
   "outputs": [],
   "source": [
    "# numpy 라이브러리 이용해서 sigmoid 함수 구현 \n",
    "import numpy as np\n",
    "\n",
    "def sigmoid_using_numpy(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c320c522",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T06:49:55.509918Z",
     "start_time": "2023-05-09T06:49:55.504909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실수 1개 test:  0.52497918747894\n",
      "vector test:  [0.52497919 0.62245933]\n"
     ]
    }
   ],
   "source": [
    "# 실수 1개 test -> 정상 동작 \n",
    "print(\"실수 1개 test: \",sigmoid_using_numpy(0.1))\n",
    "# 실수 리스트 test -> 에러 발생 \n",
    "print(\"vector test: \",sigmoid_using_numpy(np.array([0.1, 0.5])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "417e6fdb",
   "metadata": {},
   "source": [
    "### 2. gradient 계산\n",
    "* sigmoid function의 gradient(slope, derivative) 계산 \n",
    "* $sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b349bc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T06:49:56.074453Z",
     "start_time": "2023-05-09T06:49:56.065599Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_gradient(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the sigmoid function with respect to its input x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    grad -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1/(1+np.exp(-x))\n",
    "    grad = s*(1-s)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "997b45c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T06:49:56.705865Z",
     "start_time": "2023-05-09T06:49:56.689859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24937604, 0.23500371, 0.24445831])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "sigmoid_gradient(np.array([0.1, 0.5, 0.3]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4e1c7ba",
   "metadata": {},
   "source": [
    "### 3. reshape arrays\n",
    "* np.reshape을 이용하여 3D vector를 1D Vector로 \"unroll\"할 수 있음 \n",
    "* 예를 들어, 이미지 vector를 (length, height, depth) -> (lenght * height * depth, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bba2e15b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T06:49:57.946884Z",
     "start_time": "2023-05-09T06:49:57.937587Z"
    }
   },
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    vector = image.reshape(image.shape[0]*image.shape[1]*image.shape[2], 1)\n",
    "    \n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c398531",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T06:54:04.124101Z",
     "start_time": "2023-05-09T06:54:04.118046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_image shape: (3, 3, 2)\n",
      "result: [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "test_image = np.array([[[ 0.67826139,  0.29380381],\n",
    "                         [ 0.90714982,  0.52835647],\n",
    "                         [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "                       [[ 0.92814219,  0.96677647],\n",
    "                        [ 0.85304703,  0.52351845],\n",
    "                        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "                       [[ 0.60659855,  0.00533165],\n",
    "                        [ 0.10820313,  0.49978937],\n",
    "                        [ 0.34144279,  0.94630077]]])\n",
    "print(\"test_image shape:\", test_image.shape)\n",
    "print(\"result:\", image2vector(test_image))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e1ae199",
   "metadata": {},
   "source": [
    "### 4. 정규화 (Normalizing rows)\n",
    "* 정규화를 수행하면, gradient descent 수렴 속도가 빨라질 수 있음 \n",
    "* 아래 예시는 각 행마다 정규화를 수행한 모습\n",
    "* axis=1은 row-wise 방식, axis=0은 column-wise 방식\n",
    "* $\\frac{x}{\\| x\\|}$ 계산 시 broadcasting 적용된 것\n",
    "* $x = \\begin{bmatrix}\n",
    "        0 & 3 & 4 \\\\\n",
    "        2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}$\n",
    "*  \n",
    "$\\| x\\| = \\text{np.linalg.norm(x, axis=1, keepdims=True)} = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}$\n",
    "* \n",
    "$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4c6338e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T07:21:16.163911Z",
     "start_time": "2023-05-09T07:21:16.157908Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_rows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "    x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    x = x / x_norm   \n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "77ba33b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T07:21:16.446165Z",
     "start_time": "2023-05-09T07:21:16.438159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.6       , 0.8       ],\n",
       "       [0.13736056, 0.82416338, 0.54944226]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "test_x = np.array([[0, 3, 4],\n",
    "                   [1, 6, 4]])\n",
    "normalize_rows(test_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edc15d56",
   "metadata": {},
   "source": [
    "### 5. softmax 함수 구현\n",
    "* softmax 함수는 0~1사이의 값으로 모두 정규화하여 값을 리턴하며 출력 값들의 총합은 항상 1이 되는 특성을 갖는다.\n",
    "* $\\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     }$\n",
    "\n",
    "\\begin{align*}\n",
    " softmax(x) &= softmax\\left(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}\\right) \\\\&= \\begin{bmatrix}\n",
    "    \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} \n",
    "\\end{align*}\n",
    "* $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  \n",
    "\n",
    "\\begin{align*}\n",
    "softmax(x) &= softmax\\begin{bmatrix}\n",
    "            x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "            x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "            \\end{bmatrix} \\\\ \\\\&= \n",
    " \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} \\\\ \\\\ &= \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    \\vdots  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2711c022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T07:31:06.683570Z",
     "start_time": "2023-05-09T07:31:06.672569Z"
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax_func(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (m,n).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (m,n)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n",
    "    \"\"\"\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "    s = x_exp / x_sum\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7bdaaf8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T07:31:06.809119Z",
     "start_time": "2023-05-09T07:31:06.790795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.80897665e-01, 8.94462891e-04, 1.79657674e-02, 1.21052389e-04,\n",
       "        1.21052389e-04],\n",
       "       [8.78679856e-01, 1.18916387e-01, 8.01252314e-04, 8.01252314e-04,\n",
       "        8.01252314e-04]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "test_x = np.array([[9, 2, 5, 0, 0],\n",
    "                   [7, 5, 0, 0 ,0]])\n",
    "\n",
    "softmax_func(test_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1b7adb7",
   "metadata": {},
   "source": [
    "### 6. for문 사용하지 말고, vectorization !\n",
    "* dot, outer, elementwise product 등을 계산할 때 벡터화해서 계산하는 것이 효율적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b4d73dc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T07:40:20.422004Z",
     "start_time": "2023-05-09T07:40:20.407469Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7010338a",
   "metadata": {},
   "source": [
    "#### 예시 ) dot product (내적)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fdbe9b13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T07:52:48.970437Z",
     "start_time": "2023-05-09T07:52:48.955429Z"
    }
   },
   "outputs": [],
   "source": [
    "### dot product ###\n",
    "# 하나씩 계산해서 더함 \n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot += x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "# print (\"dot = \" + str(dot) + \"\\n계산시간 = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "# 벡터화\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "# print (\"dot = \" + str(dot) + \"\\n계산시간 = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### outer product ###\n",
    "# 하나씩 계산해서 더함 \n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1), len(x2)))\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i] * x2[j]\n",
    "toc = time.process_time()\n",
    "# print(\"\\n\")\n",
    "# print (\"dot = \" + str(outer) + \"\\n계산시간 = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "# 벡터화\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "# print (\"dot = \" + str(outer) + \"\\n계산시간 = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### elementwise ###\n",
    "# 하나씩 계산해서 더함 \n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "# print(\"\\n\")\n",
    "# print (\"dot = \" + str(mul) + \"\\n계산시간 = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "# 벡터화 \n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "# print (\"dot = \" + str(mul) + \"\\n계산시간 = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### general dot product ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "\n",
    "# 하나씩 계산해서 더함 \n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j] * x1[j]\n",
    "toc = time.process_time()\n",
    "# print(\"\\n\")\n",
    "# print (\"dot = \" + str(gdot) + \"\\n계산시간 = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "# 벡터화 \n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "# print (\"dot = \" + str(dot) + \"\\n계산시간 = \" + str(1000 * (toc - tic)) + \"ms\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "209f30a9",
   "metadata": {},
   "source": [
    "### 7. loss function 구현\n",
    "* L1 loss\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^{m-1}|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}$$\n",
    "* L2 loss\n",
    "$$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^{m-1}(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f334c212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T07:52:49.800334Z",
     "start_time": "2023-05-09T07:52:49.787028Z"
    }
   },
   "outputs": [],
   "source": [
    "def L1(y, yhat):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L1 loss function defined above\n",
    "    \"\"\"\n",
    "    loss = np.sum(abs(y-yhat))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "01312e8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T07:52:50.392536Z",
     "start_time": "2023-05-09T07:52:50.386535Z"
    }
   },
   "outputs": [],
   "source": [
    "def L2(y, yhat):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L2 loss function defined above\n",
    "    \"\"\"\n",
    "\n",
    "    loss = np.sum(np.dot(y-yhat, y-yhat))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544c5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaret_py38",
   "language": "python",
   "name": "pycaret_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
